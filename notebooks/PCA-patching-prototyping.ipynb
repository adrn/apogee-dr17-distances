{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['APOGEE_CACHE_PATH'] = \"/mnt/ceph/users/apricewhelan/apogee-test/\"\n",
    "os.environ['JOAQUIN_CACHE_PATH'] = \"/mnt/ceph/users/apricewhelan/projects/joaquin/cache\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning) \n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import pathlib\n",
    "_path = str(pathlib.Path('../').resolve())\n",
    "if _path not in sys.path:\n",
    "    sys.path.append(_path)\n",
    "\n",
    "import corner\n",
    "from astropy.io import fits\n",
    "import astropy.coordinates as coord\n",
    "import astropy.table as at\n",
    "import astropy.units as u\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from joaquin.data import JoaquinData\n",
    "from joaquin.config import dr, root_cache_path, neighborhood_size, testing_zone_size\n",
    "from joaquin.plot import simple_corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_n_components = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = pathlib.Path(f'../cache/{dr}').resolve()\n",
    "cache_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "plot_path = pathlib.Path('../plot') / dr\n",
    "plot_path = plot_path.resolve()\n",
    "plot_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the first two notebooks (1- and 2-) to set up the necessary files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_data = JoaquinData(\n",
    "    cache_file='parent-sample-raw')\n",
    "\n",
    "spec_good_mask = np.load(root_cache_path / 'spec_good_mask.npy')\n",
    "\n",
    "neighborhood_idx = np.load(cache_path / 'good_parent_neighborhood_indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_stars = parent_data.stars[parent_data.stars_mask]\n",
    "parent_d, *_ = parent_data.get_Xy(spec_mask_thresh=1.)  # disable spec mask\n",
    "assert len(parent_stars) == parent_d['X'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in neighborhood_idx[10:]:\n",
    "    pca = PCA(n_components=patching_n_components)\n",
    "    subX = spec_parent_d['X'][idx].copy()\n",
    "    subX[:, ~spec_good_mask] = 0.\n",
    "    \n",
    "    subX_pca = pca.fit_transform(subX)\n",
    "    tmp_patched = pca.inverse_transform(subX_pca)\n",
    "    \n",
    "    subX_patched = subX.copy()\n",
    "    subX_patched[subX_patched == 0] = tmp_patched[subX_patched == 0]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix_fixed = (subX[:, spec_good_mask] == 0).sum()\n",
    "assert (subX_patched[:, spec_good_mask] == 0).sum() == 0\n",
    "\n",
    "print(f\"{npix_fixed} pixels patched, ~{npix_fixed/subX.shape[0]:.0f} pixels per star\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    tmp = subX[i, :].copy()\n",
    "    tmp[~spec_good_mask] = np.nan\n",
    "    plt.plot(tmp, marker='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    tmp = subX_patched[i, :].copy()\n",
    "    tmp[~spec_good_mask] = np.nan\n",
    "    plt.plot(tmp, marker='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joaquin.filters import nufft_lowpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (parent_data.stars[idx]['SNR'] < 60) & (parent_data.stars[idx]['SNR'] > 40)\n",
    "i = np.where(foo)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 4\n",
    "\n",
    "new_ln_flux = nufft_lowpass(np.log(parent_data._X_wvln), \n",
    "                            subX_patched[i],\n",
    "                            fcut=0.5 * 22500,\n",
    "                            bad_mask=~spec_good_mask)\n",
    "new_ln_flux[~spec_good_mask] = np.nan\n",
    "\n",
    "for lim in [False, 'zoom', 'zoomer']:\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.plot(parent_data._X_wvln, subX[i], marker='', drawstyle='steps-mid')\n",
    "    plt.plot(parent_data._X_wvln, subX_patched[i], marker='', drawstyle='steps-mid')\n",
    "    plt.plot(parent_data._X_wvln, new_ln_flux, marker='', drawstyle='steps-mid')\n",
    "    if lim == 'zoom':\n",
    "        plt.xlim(16000, 16500)\n",
    "    elif lim == 'zoomer':\n",
    "        plt.xlim(16150, 16220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try running the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training sample is the full neighborhood, with some parallax and S/N cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_stars = parent_stars[idx]\n",
    "neighborhood_X = np.hstack((subX_patched[:, spec_good_mask], \n",
    "                            lsfphot_parent_d['X'][idx]))\n",
    "\n",
    "test_stars = neighborhood_stars[:testing_zone_size]\n",
    "test_X = neighborhood_X[:testing_zone_size]\n",
    "\n",
    "len(neighborhood_stars), len(test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = ((neighborhood_stars['SNR'] > 100) &\n",
    "              (neighborhood_stars['GAIAEDR3_PARALLAX_ERROR'] < 0.1))\n",
    "# TODO: add RUWE selection\n",
    "\n",
    "test_mask = ((test_stars['SNR'] > 100) &\n",
    "             (test_stars['GAIAEDR3_PARALLAX_ERROR'] < 0.1))\n",
    "validate_mask = (\n",
    "    (test_stars['SNR'] > 100) &\n",
    "    ((test_stars['GAIAEDR3_PARALLAX'] / test_stars['GAIAEDR3_PARALLAX_ERROR']) > 20))\n",
    "\n",
    "train_mask.sum(), test_mask.sum(), validate_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.5, 5, 256)\n",
    "plt.hist(neighborhood_stars['GAIAEDR3_PARALLAX'][train_mask], \n",
    "         bins=bins);\n",
    "\n",
    "plt.hist(test_stars['GAIAEDR3_PARALLAX'][test_mask], \n",
    "         bins=bins);\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.transform import downscale_local_mean\n",
    "\n",
    "# tmp_X = downscale_local_mean(\n",
    "#     neighborhood_X[train_mask],\n",
    "#     (4, 4))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "# ax.imshow(tmp_X, \n",
    "#           origin='lower', \n",
    "#           vmin=np.percentile(tmp_X, 5), \n",
    "#           vmax=np.percentile(tmp_X, 95))\n",
    "# # ax.set_aspect(2)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joaquin:\n",
    "\n",
    "    def __init__(self, X, y, y_ivar, idx_map, frozen=None):\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        self.y_ivar = y_ivar\n",
    "\n",
    "        # Currently, stores parameter names and shapes\n",
    "        self._param_info = {}\n",
    "\n",
    "        # duh\n",
    "        self._param_info['parallax_zpt'] = 1\n",
    "\n",
    "        # the inv-var of the prior on the spectral components in beta\n",
    "        self._param_info['L2_ivar'] = 1\n",
    "\n",
    "        # linear coefficients (in the exp argument)\n",
    "        self._param_info['beta'] = self.X.shape[1]\n",
    "\n",
    "        if 'spec' in terms:\n",
    "            L2_slice = self.idx_map['spec']\n",
    "        else:\n",
    "            L2_slice = np.ones(self.X.shape[1], dtype=bool)\n",
    "        self.L2_slice = L2_slice\n",
    "\n",
    "        if frozen is None:\n",
    "            frozen = {}\n",
    "        self.frozen = frozen\n",
    "\n",
    "    def unpack_pars(self, par_list):\n",
    "        i = 0\n",
    "        par_dict = {}\n",
    "        for key, par_len in self._param_info.items():\n",
    "            if key in self.frozen:\n",
    "                par_dict[key] = self.frozen[key]\n",
    "            else:\n",
    "                par_dict[key] = np.array(par_list[i:i+par_len])\n",
    "                if len(par_dict[key]) == 1:  # HORRIBLE\n",
    "                    par_dict[key] = par_dict[key][0]\n",
    "\n",
    "                i += par_len\n",
    "\n",
    "        return par_dict\n",
    "\n",
    "    def pack_pars(self, par_dict):\n",
    "        parvec = []\n",
    "        for i, k in enumerate(self._param_info):\n",
    "            if k not in self.frozen:\n",
    "                parvec.append(par_dict[k])\n",
    "        return np.concatenate(parvec)\n",
    "\n",
    "    def init_beta(self, parallax_zpt=None, L2_ivar=None):\n",
    "        parallax_zpt = self.frozen.get('parallax_zpt', parallax_zpt)\n",
    "        L2_ivar = self.frozen.get('L2_ivar', L2_ivar)\n",
    "\n",
    "        if parallax_zpt is None or L2_ivar is None:\n",
    "            raise ValueError('todo')\n",
    "\n",
    "        y = self.y + parallax_zpt\n",
    "        plx_mask = y > (3 / np.sqrt(self.y_ivar))  # 3 sigma\n",
    "\n",
    "        X = self.X[plx_mask]\n",
    "        y = y[plx_mask]\n",
    "        y_ivar = self.y_ivar[plx_mask]\n",
    "\n",
    "        ln_plx_ivar = y**2 * y_ivar\n",
    "        ln_y = np.log(y)\n",
    "\n",
    "        XT_Cinv = X.T * ln_plx_ivar\n",
    "        XT_Cinv_X = np.dot(XT_Cinv, X)\n",
    "        XT_Cinv_X[np.diag_indices(X.shape[1])] += L2_ivar\n",
    "\n",
    "        beta = np.linalg.solve(XT_Cinv_X, np.dot(XT_Cinv, ln_y))\n",
    "        return beta\n",
    "\n",
    "    def chi(self, parallax_zpt, L2_ivar, beta):\n",
    "        y = self.y + parallax_zpt\n",
    "        model_ln_plx = np.dot(self.X, beta)\n",
    "        model_y = np.exp(model_ln_plx)\n",
    "        resid = y - model_y\n",
    "        return resid * np.sqrt(self.y_ivar)\n",
    "\n",
    "    def ln_likelihood(self, parallax_zpt, L2_ivar, beta):\n",
    "        y = self.y + parallax_zpt\n",
    "        model_ln_plx = np.dot(self.X, beta)\n",
    "        model_y = np.exp(model_ln_plx)\n",
    "        resid = y - model_y\n",
    "\n",
    "        ll = -0.5 * np.sum(resid**2 * self.y_ivar)\n",
    "        ll_grad = np.dot(self.X.T,\n",
    "                         model_y * self.y_ivar * resid)\n",
    "\n",
    "        return ll, ll_grad\n",
    "\n",
    "    def ln_prior(self, parallax_zpt, L2_ivar, beta):\n",
    "        lp = - 0.5 * L2_ivar * np.sum(beta[self.L2_slice] ** 2)\n",
    "        lp_grad = np.zeros_like(beta)\n",
    "        lp_grad[self.L2_slice] = - L2_ivar * beta[self.L2_slice]\n",
    "        return lp, lp_grad\n",
    "\n",
    "    def neg_ln_posterior(self, parallax_zpt, L2_ivar, beta):\n",
    "        ll, ll_grad = self.ln_likelihood(parallax_zpt, L2_ivar, beta)\n",
    "        lp, lp_grad = self.ln_prior(parallax_zpt, L2_ivar, beta)\n",
    "        logger.log(0, f'objective function evaluation: ll={ll}, lp={lp}')\n",
    "        return - (ll + lp), - (ll_grad + lp_grad)\n",
    "\n",
    "    def __call__(self, p):\n",
    "        par_dict = self.unpack_pars(p)\n",
    "        return self.neg_ln_posterior(**par_dict)\n",
    "\n",
    "    def optimize(self, init=None, **minimize_kwargs):\n",
    "        \"\"\"\n",
    "        To set the maximum number of function evaluations, pass:\n",
    "\n",
    "            options={'maxfun': ...}\n",
    "\n",
    "        \"\"\"\n",
    "        if init is None:\n",
    "            init = {}\n",
    "\n",
    "        init.setdefault('parallax_zpt', 0.)\n",
    "        init.setdefault('L2_ivar', 1.)\n",
    "\n",
    "        if 'beta' not in init:\n",
    "            init['beta'] = self.init_beta(**init)\n",
    "\n",
    "        x0 = self.pack_pars(init)\n",
    "\n",
    "        minimize_kwargs.setdefault('method', 'L-BFGS-B')\n",
    "        if minimize_kwargs['method'] == 'L-BFGS-B':\n",
    "            minimize_kwargs.setdefault('options', {'maxfun': 1024})\n",
    "\n",
    "        res = minimize(\n",
    "            self,\n",
    "            x0=x0,\n",
    "            jac=True,\n",
    "            **minimize_kwargs)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Kfold_indices(stars, K, rng=None):\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "    idx = np.arange(len(stars))\n",
    "    rng.shuffle(idx)\n",
    "    \n",
    "    batch_size = len(stars) // K\n",
    "    train_batches = []\n",
    "    test_batches = []\n",
    "    for k in range(K):\n",
    "        if k == K-1:\n",
    "            batch = idx[k*batch_size:]\n",
    "        else:\n",
    "            batch = idx[k*batch_size:(k+1)*batch_size]\n",
    "            \n",
    "        test_batches.append(batch)\n",
    "        train_batches.append(idx[~np.isin(idx, batch)])\n",
    "        \n",
    "    assert np.all(np.array([len(train_batches[i]) + len(test_batches[i]) \n",
    "                            for i in range(len(train_batches))]) == len(stars))\n",
    "    \n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_K_batches(data, K, frozen=None, optimize_kw=None):\n",
    "    \"\"\"\n",
    "    TODO: could take a pool argument and parallelize the loop below\n",
    "    \"\"\"\n",
    "\n",
    "    if frozen is None:\n",
    "        frozen = dict()\n",
    "\n",
    "    if optimize_kw is None:\n",
    "        optimize_kw = dict()\n",
    "    optimize_kw.setdefault('options', {'maxiter': 1_000})  # TODO: make this bigger\n",
    "\n",
    "    train_batches, test_batches = get_Kfold_indices(clean_stars, K=K)\n",
    "\n",
    "    batch_fit_pars = []\n",
    "    batch_res = []\n",
    "    test_loss = []\n",
    "    for k, (train_batch, test_batch) in enumerate(zip(train_batches, test_batches)):\n",
    "        joa = Joaquin(data[train_batch], frozen=frozen)\n",
    "        test_joa = Joaquin(data[test_batch], frozen=frozen)\n",
    "        \n",
    "        res = joa.optimize(**optimize_kw)\n",
    "        fit_pars = joa.unpack_pars(res.x)\n",
    "\n",
    "        batch_res.append(res)\n",
    "        batch_fit_pars.append(fit_pars)\n",
    "        \n",
    "        # evaluate the fit model on the test batch\n",
    "        test_loss.append(test_joa.neg_ln_posterior(**fit_pars)[0])\n",
    "\n",
    "    return batch_fit_pars, batch_res, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def cross_validate_hyperpars(data, K, frozen, **kwargs):\n",
    "    kwargs = kwargs.copy()\n",
    "    kwargs.setdefault('method', 'powell')\n",
    "    \n",
    "    # HACK / BAD: hardcoded names\n",
    "    assert len(frozen) == 1\n",
    "    if 'L2_ivar' in frozen:\n",
    "        xval_par = 'parallax_zpt'\n",
    "        kwargs.setdefault('x0', -0.03)\n",
    "        \n",
    "    elif 'parallax_zpt' in frozen:\n",
    "        xval_par = 'L2_ivar'\n",
    "        kwargs.setdefault('x0', 1e2)\n",
    "    \n",
    "    def objective(p):\n",
    "        pars = frozen.copy()\n",
    "        pars[xval_par] = p\n",
    "        fit_pars, reses, losses = fit_K_batches(data, K, frozen=pars)\n",
    "        return sum(losses)\n",
    "    \n",
    "    res = minimize(objective, **kwargs)\n",
    "    return {xval_par: float(res.x)}, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrian conda base",
   "language": "python",
   "name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
